"""
Processeur PDF optimis√© pour l'extraction de texte avec m√©tadonn√©es
"""
import os
import pdfplumber
from datetime import datetime
from typing import List, Dict, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from config.settings import settings
from config.logging_config import get_logger
from core.exceptions import PDFProcessingError
from data.validators import PDFValidator, validate_pdf_batch, DataValidator
from utils.helpers import format_file_size, format_timestamp, measure_execution_time

logger = get_logger(__name__)

class PDFProcessor:
    """
    Processeur PDF robuste avec extraction de m√©tadonn√©es
    
    Fonctionnalit√©s:
    - Extraction de texte avec m√©tadonn√©es d√©taill√©es
    - Validation automatique des fichiers
    - Traitement parall√®le (optionnel)
    - Gestion d'erreurs avanc√©e
    - Statistiques de traitement
    """
    
    def __init__(self, validate_files: bool = True, parallel_processing: bool = False):
        """
        Initialise le processeur PDF
        
        Args:
            validate_files: Valider les fichiers avant traitement
            parallel_processing: Utiliser le traitement parall√®le
        """
        self.validate_files = validate_files
        self.parallel_processing = parallel_processing
        self.processing_stats = {
            'total_files': 0,
            'successful_files': 0,
            'failed_files': 0,
            'total_pages': 0,
            'total_characters': 0,
            'processing_time': 0.0
        }
    
    @measure_execution_time
    def extract_text_from_pdf(self, pdf_path: str) -> Tuple[str, Dict, Dict]:
        """
        Extrait le texte d'un seul fichier PDF avec m√©tadonn√©es
        
        Args:
            pdf_path: Chemin vers le fichier PDF
            
        Returns:
            Tuple (texte_extrait, m√©tadonn√©es, statistiques_document)
            
        Raises:
            PDFProcessingError: Si l'extraction √©choue
        """
        if self.validate_files:
            is_valid, validation_report = PDFValidator.validate_pdf_file(pdf_path)
            if not is_valid:
                errors = validation_report.get('validation_errors', ['Validation failed'])
                raise PDFProcessingError(f"PDF invalid: {'; '.join(errors)}")
        
        try:
            text_content = ""
            metadata = {}
            doc_name = os.path.basename(pdf_path)
            chunk_counter = 0
            page_count = 0
            total_chars = 0
            
            with pdfplumber.open(pdf_path) as pdf:
                logger.info(f"Processing PDF: {doc_name} ({len(pdf.pages)} pages)")
                
                for page_num, page in enumerate(pdf.pages, 1):
                    try:
                        # Extraction am√©lior√©e avec options
                        page_text = page.extract_text(
                            x_tolerance=2,  # Tol√©rance horizontale
                            y_tolerance=3,  # Tol√©rance verticale
                            layout=True,    # Pr√©server la mise en page
                            x_density=7.25, # Densit√© pour grouper les caract√®res
                            y_density=13    # Densit√© pour les lignes
                        )
                        
                        if page_text and len(page_text.strip()) >= settings.MIN_PAGE_CONTENT_LENGTH:
                            # Nettoyer le texte
                            cleaned_text = self._clean_extracted_text(page_text)
                            
                            # Cr√©er les m√©tadonn√©es pour cette page
                            page_metadata = {
                                'document': doc_name,
                                'page': page_num,
                                'text_length': len(cleaned_text),
                                'extraction_time': datetime.now().isoformat(),
                                'file_path': pdf_path,
                                'file_size': os.path.getsize(pdf_path)
                            }
                            
                            # Ajouter les m√©tadonn√©es au dictionnaire global
                            metadata[chunk_counter] = page_metadata
                            
                            # Ajouter le texte avec marqueurs de m√©tadonn√©es
                            text_content += f"\n[DOC_META_{chunk_counter}]{cleaned_text}[/DOC_META_{chunk_counter}]\n"
                            
                            chunk_counter += 1
                            page_count += 1
                            total_chars += len(cleaned_text)
                        
                        else:
                            logger.debug(f"Page {page_num} skipped (insufficient content)")
                    
                    except Exception as e:
                        logger.warning(f"Error extracting page {page_num} from {doc_name}: {e}")
                        continue
                
                # Statistiques du document
                doc_stats = {
                    'total_pages': len(pdf.pages),
                    'processed_pages': page_count,
                    'total_characters': total_chars,
                    'file_size': os.path.getsize(pdf_path),
                    'processing_time': datetime.now().isoformat()
                }
                
                if page_count > 0:
                    logger.info(
                        f"‚úÖ {doc_name}: {page_count}/{len(pdf.pages)} pages processed, "
                        f"{total_chars} characters extracted"
                    )
                else:
                    logger.warning(f"‚ö†Ô∏è {doc_name}: No readable content found")
                
                return text_content, metadata, doc_stats
                
        except Exception as e:
            error_msg = f"Critical error processing {os.path.basename(pdf_path)}: {e}"
            logger.error(error_msg)
            raise PDFProcessingError(error_msg)
    
    def extract_with_enriched_metadata(self, pdf_path: str) -> Tuple[str, Dict, Dict]:
        """
        Extrait le texte d'un PDF avec m√©tadonn√©es enrichies pour tra√ßabilit√©
        
        Args:
            pdf_path: Chemin vers le fichier PDF
            
        Returns:
            Tuple (texte_extrait, m√©tadonn√©es_enrichies, statistiques_document)
        """
        if self.validate_files:
            is_valid, validation_report = PDFValidator.validate_pdf_file(pdf_path)
            if not is_valid:
                errors = validation_report.get('validation_errors', ['Validation failed'])
                raise PDFProcessingError(f"PDF invalid: {'; '.join(errors)}")
        
        try:
            text_content = ""
            metadata = {}
            doc_name = os.path.basename(pdf_path)
            chunk_counter = 0
            page_count = 0
            total_chars = 0
            
            # D√©terminer le type de guideline depuis le nom de fichier
            guideline_type = self._detect_guideline_type(doc_name)
            
            # Charger la liste des mol√©cules pour la d√©tection
            molecule_list = self._load_molecule_list_for_detection()
            
            with pdfplumber.open(pdf_path) as pdf:
                logger.info(f"Processing PDF with enriched metadata: {doc_name} ({len(pdf.pages)} pages)")
                
                for page_num, page in enumerate(pdf.pages, 1):
                    try:
                        # Extraction am√©lior√©e avec options
                        page_text = page.extract_text(
                            x_tolerance=2,  # Tol√©rance horizontale
                            y_tolerance=3,  # Tol√©rance verticale
                            layout=True,    # Pr√©server la mise en page
                            x_density=7.25, # Densit√© pour grouper les caract√®res
                            y_density=13    # Densit√© pour les lignes
                        )
                        
                        if page_text and len(page_text.strip()) >= settings.MIN_PAGE_CONTENT_LENGTH:
                            # Nettoyer le texte
                            cleaned_text = self._clean_extracted_text(page_text)
                            
                            # D√©tection automatique des sections
                            section_info = self._detect_sections(cleaned_text, page_num)
                            
                            # D√©tection des mentions de mol√©cules
                            drug_mentions = self._detect_drug_mentions(cleaned_text, molecule_list)
                            
                            # Mots-cl√©s d'interaction (bas√©s sur les mol√©cules d√©tect√©es)
                            interaction_keywords = drug_mentions  # Les mol√©cules sont les mots-cl√©s
                            
                            # Score de confiance bas√© sur la qualit√© du texte et des d√©tections
                            confidence_score = self._calculate_confidence_score(
                                cleaned_text, drug_mentions, section_info
                            )
                            
                            # Cr√©er les m√©tadonn√©es enrichies pour cette page
                            page_metadata = {
                                'source_file': doc_name,
                                'page': page_num,
                                'section_title': section_info['title'],
                                'section_type': section_info['type'],
                                'drug_mentions': drug_mentions,
                                'interaction_keywords': interaction_keywords,
                                'guideline_type': guideline_type,
                                'confidence_score': confidence_score,
                                'text_length': len(cleaned_text),
                                'extraction_time': datetime.now().isoformat(),
                                'file_path': pdf_path,
                                'file_size': os.path.getsize(pdf_path),
                                'exact_quote_context': self._prepare_quote_context(cleaned_text)
                            }
                            
                            # Ajouter les m√©tadonn√©es au dictionnaire global
                            metadata[chunk_counter] = page_metadata
                            
                            # Ajouter le texte avec marqueurs de m√©tadonn√©es
                            text_content += f"\n[DOC_META_{chunk_counter}]{cleaned_text}[/DOC_META_{chunk_counter}]\n"
                            
                            chunk_counter += 1
                            page_count += 1
                            total_chars += len(cleaned_text)
                        
                        else:
                            logger.debug(f"Page {page_num} skipped (insufficient content)")
                    
                    except Exception as e:
                        logger.warning(f"Error extracting page {page_num} from {doc_name}: {e}")
                        continue
                
                # Statistiques du document
                doc_stats = {
                    'total_pages': len(pdf.pages),
                    'processed_pages': page_count,
                    'total_characters': total_chars,
                    'file_size': os.path.getsize(pdf_path),
                    'processing_time': datetime.now().isoformat(),
                    'guideline_type': guideline_type,
                    'total_drug_mentions': sum(len(meta.get('drug_mentions', [])) for meta in metadata.values()),
                    'avg_confidence_score': sum(meta.get('confidence_score', 0) for meta in metadata.values()) / max(len(metadata), 1)
                }
                
                if page_count > 0:
                    logger.info(
                        f"‚úÖ {doc_name}: {page_count}/{len(pdf.pages)} pages processed, "
                        f"{total_chars} characters extracted, "
                        f"{doc_stats['total_drug_mentions']} drug mentions found"
                    )
                else:
                    logger.warning(f"‚ö†Ô∏è {doc_name}: No readable content found")
                
                return text_content, metadata, doc_stats
                
        except Exception as e:
            error_msg = f"Critical error processing {os.path.basename(pdf_path)}: {e}"
            logger.error(error_msg)
            raise PDFProcessingError(error_msg)
    
    def _clean_extracted_text(self, text: str) -> str:
        """
        Nettoie le texte extrait du PDF de mani√®re plus robuste
        
        Args:
            text: Texte brut extrait
            
        Returns:
            Texte nettoy√© et bien structur√©
        """
        if not text:
            return ""
        
        import re
        
        # Supprimer les caract√®res de contr√¥le mais garder les sauts de ligne
        text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x84\x86-\x9f]', '', text)
        
        # Corriger les mots coup√©s (ex: "word-\nword" -> "wordword")
        text = re.sub(r'(\w+)-\s*\n\s*(\w+)', r'\1\2', text)
        
        # Normaliser les espaces mais garder la structure
        text = re.sub(r'[ \t]+', ' ', text)  # Espaces multiples -> un seul
        
        # Garder max 2 sauts de ligne cons√©cutifs
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        
        # Nettoyer les tirets de bullet points mal format√©s
        text = re.sub(r'^\s*[-‚Ä¢¬∑]\s*', '‚Ä¢ ', text, flags=re.MULTILINE)
        
        # Corriger les num√©rotations cass√©es
        text = re.sub(r'(\d+)\s*\.\s*([A-Z])', r'\1. \2', text)
        
        return text.strip()
    
    def _detect_guideline_type(self, filename: str) -> str:
        """
        D√©termine le type de guideline depuis le nom de fichier
        
        Args:
            filename: Nom du fichier PDF
            
        Returns:
            Type de guideline d√©tect√©
        """
        filename_lower = filename.lower()
        
        # Patterns de d√©tection
        if 'stopp' in filename_lower or 'start' in filename_lower:
            return 'STOPP/START'
        elif 'beers' in filename_lower or 'ags' in filename_lower:
            return 'BEERS_CRITERIA'
        elif 'laroche' in filename_lower:
            return 'LAROCHE_LIST'
        elif 'priscus' in filename_lower:
            return 'PRISCUS_LIST'
        elif 'onc' in filename_lower or 'ddi' in filename_lower:
            return 'ONC_DDI'
        else:
            return 'UNKNOWN_GUIDELINE'
    
    def _load_molecule_list_for_detection(self) -> set:
        """
        Charge la liste des mol√©cules depuis le CSV pour la d√©tection
        
        Returns:
            Set des noms de mol√©cules (normalis√©s)
        """
        import pandas as pd
        
        csv_path = os.path.join(
            os.path.dirname(os.path.dirname(__file__)),
            "data", "mimic_molecule", "molecule_unique_mimic.csv"
        )
        
        molecules = set()
        
        try:
            if os.path.exists(csv_path):
                df = pd.read_csv(csv_path)
                
                if 'Mol√©cule' in df.columns:
                    for molecule in df['Mol√©cule'].dropna():
                        clean_name = molecule.strip().lower()
                        if clean_name:
                            molecules.add(clean_name)
                    
                    logger.debug(f"Loaded {len(molecules)} molecules for detection")
                else:
                    logger.warning(f"Column 'Mol√©cule' not found in CSV")
            else:
                logger.warning(f"Molecule CSV not found at: {csv_path}")
                
        except Exception as e:
            logger.error(f"Error loading molecule list for detection: {e}")
            
        return molecules
    
    def _detect_sections(self, text: str, page_num: int) -> Dict[str, str]:
        """
        D√©tecte automatiquement les sections dans le texte
        
        Args:
            text: Texte de la page
            page_num: Num√©ro de page
            
        Returns:
            Informations sur la section
        """
        import re
        
        # Patterns pour d√©tecter les sections
        section_patterns = {
            'interaction': r'\b(interaction|drug.{0,10}interaction|contraindication)\b',
            'dosage': r'\b(dosage|dose|posologie|administration)\b',
            'indication': r'\b(indication|treatment|traitement|th√©rapie)\b',
            'warning': r'\b(warning|attention|avertissement|pr√©caution)\b',
            'contraindication': r'\b(contraindication|contre.indication)\b',
            'adverse_effect': r'\b(adverse|effect|side.effect|effet.ind√©sirable)\b'
        }
        
        # Chercher les titres potentiels (lignes courtes en majuscules ou avec formatting sp√©cial)
        lines = text.split('\n')
        potential_titles = []
        
        for line in lines[:10]:  # Analyser les 10 premi√®res lignes
            line = line.strip()
            if len(line) < 100 and len(line) > 5:  # Titre potentiel
                if line.isupper() or line.count(' ') < 5:  # Titre en majuscules ou court
                    potential_titles.append(line)
        
        # D√©terminer le type de section
        text_lower = text.lower()
        section_type = 'general'
        
        for section_name, pattern in section_patterns.items():
            if re.search(pattern, text_lower, re.IGNORECASE):
                section_type = section_name
                break
        
        # Titre de section (prendre le premier titre potentiel ou g√©n√©rer)
        section_title = potential_titles[0] if potential_titles else f"Page {page_num}"
        
        return {
            'title': section_title[:100],  # Limiter la longueur
            'type': section_type
        }
    
    def _detect_drug_mentions(self, text: str, molecule_list: set) -> List[str]:
        """
        D√©tecte les mentions de mol√©cules dans le texte
        
        Args:
            text: Texte √† analyser
            molecule_list: Liste des mol√©cules connues
            
        Returns:
            Liste des mol√©cules d√©tect√©es
        """
        import re
        
        found_molecules = set()
        text_lower = text.lower()
        
        # Recherche exacte dans la liste CSV
        for molecule in molecule_list:
            molecule_clean = molecule.strip()
            if not molecule_clean:
                continue
                
            # Pattern de correspondance exacte (mot entier)
            pattern = r'\b' + re.escape(molecule_clean) + r'\b'
            if re.search(pattern, text_lower):
                # Retrouver la forme originale
                found_molecules.add(molecule_clean.title())  # Mettre en forme title case
        
        return sorted(list(found_molecules))
    
    def _calculate_confidence_score(self, text: str, drug_mentions: List[str], section_info: Dict) -> float:
        """
        Calcule un score de confiance pour le contenu extrait
        
        Args:
            text: Texte extrait
            drug_mentions: Mol√©cules d√©tect√©es
            section_info: Informations sur la section
            
        Returns:
            Score de confiance entre 0 et 1
        """
        score = 0.0
        
        # Facteur 1: Longueur du texte (plus c'est long, plus c'est fiable)
        length_score = min(len(text) / 1000, 1.0)  # Normaliser √† 1000 caract√®res
        score += length_score * 0.3
        
        # Facteur 2: Nombre de mol√©cules d√©tect√©es
        drug_score = min(len(drug_mentions) / 5, 1.0)  # Normaliser √† 5 mol√©cules
        score += drug_score * 0.4
        
        # Facteur 3: Type de section (certaines sections sont plus fiables)
        section_scores = {
            'interaction': 1.0,
            'contraindication': 0.9,
            'warning': 0.8,
            'indication': 0.7,
            'dosage': 0.6,
            'adverse_effect': 0.5,
            'general': 0.3
        }
        section_score = section_scores.get(section_info.get('type', 'general'), 0.3)
        score += section_score * 0.3
        
        return round(min(score, 1.0), 3)  # Assurer que le score reste entre 0 et 1
    
    def _prepare_quote_context(self, text: str) -> Dict[str, str]:
        """
        Pr√©pare le contexte pour les citations exactes (100 caract√®res avant/apr√®s)
        
        Args:
            text: Texte complet
            
        Returns:
            Contexte pr√©par√© pour les citations
        """
        # Pour l'instant, retourner le texte complet
        # L'extraction de citation exacte se fera lors de la recherche
        return {
            'full_text': text,
            'prepared_for_citation': True
        }
    
    @measure_execution_time
    def extract_text_from_multiple_pdfs(self, pdf_paths: List[str]) -> Tuple[str, Dict, Dict]:
        """
        Extrait le texte de plusieurs fichiers PDF
        
        Args:
            pdf_paths: Liste des chemins vers les fichiers PDF
            
        Returns:
            Tuple (texte_combin√©, m√©tadonn√©es_combin√©es, statistiques_globales)
        """
        if not pdf_paths:
            logger.warning("No PDF files provided for processing")
            return "", {}, self.processing_stats
        
        # Validation par lot si demand√©e
        if self.validate_files:
            validation_report = validate_pdf_batch(pdf_paths)
            valid_files = [
                path for path, details in validation_report['validation_details'].items()
                if not details['validation_errors']
            ]
            
            if not valid_files:
                raise PDFProcessingError("No valid PDF files found after validation")
            
            logger.info(f"Validation passed: {len(valid_files)}/{len(pdf_paths)} files valid")
            pdf_paths = valid_files
        
        # R√©initialiser les statistiques
        self.processing_stats = {
            'total_files': len(pdf_paths),
            'successful_files': 0,
            'failed_files': 0,
            'total_pages': 0,
            'total_characters': 0,
            'processing_time': 0.0
        }
        
        combined_text = ""
        combined_metadata = {}
        chunk_offset = 0
        
        if self.parallel_processing and len(pdf_paths) > 1:
            # Traitement parall√®le (garde la m√©thode normale)
            results = self._process_pdfs_parallel(pdf_paths)
        else:
            # Traitement s√©quentiel avec m√©tadonn√©es enrichies
            results = self._process_pdfs_sequential_enriched(pdf_paths)
        
        # Combiner les r√©sultats
        for pdf_path, result in results.items():
            if result['success']:
                text, metadata, doc_stats = result['data']
                
                # Ajouter le texte
                combined_text += text
                
                # R√©ajuster les cl√©s des m√©tadonn√©es pour √©viter les conflits
                for original_key, meta_data in metadata.items():
                    new_key = chunk_offset + original_key
                    combined_metadata[new_key] = meta_data
                
                chunk_offset += len(metadata)
                
                # Mettre √† jour les statistiques
                self.processing_stats['successful_files'] += 1
                self.processing_stats['total_pages'] += doc_stats['processed_pages']
                self.processing_stats['total_characters'] += doc_stats['total_characters']
            
            else:
                self.processing_stats['failed_files'] += 1
                logger.error(f"Failed to process {os.path.basename(pdf_path)}: {result['error']}")
        
        logger.info(
            f"PDF processing completed: {self.processing_stats['successful_files']}/{self.processing_stats['total_files']} files successful, "
            f"{self.processing_stats['total_pages']} pages, {self.processing_stats['total_characters']} characters"
        )
        
        return combined_text, combined_metadata, self.processing_stats
    
    def _process_pdfs_sequential(self, pdf_paths: List[str]) -> Dict[str, Dict]:
        """Traitement s√©quentiel des PDF"""
        results = {}
        
        for pdf_path in pdf_paths:
            try:
                text, metadata, doc_stats = self.extract_text_from_pdf(pdf_path)
                results[pdf_path] = {
                    'success': True,
                    'data': (text, metadata, doc_stats)
                }
            except Exception as e:
                results[pdf_path] = {
                    'success': False,
                    'error': str(e)
                }
        
        return results
    
    def _process_pdfs_sequential_enriched(self, pdf_paths: List[str]) -> Dict[str, Dict]:
        """Traitement s√©quentiel avec m√©tadonn√©es enrichies"""
        results = {}
        
        for i, pdf_path in enumerate(pdf_paths, 1):
            try:
                logger.info(f"Processing PDF {i}/{len(pdf_paths)}: {os.path.basename(pdf_path)}")
                text, metadata, doc_stats = self.extract_with_enriched_metadata(pdf_path)
                results[pdf_path] = {
                    'success': True,
                    'data': (text, metadata, doc_stats)
                }
                logger.info(f"‚úÖ PDF {i}/{len(pdf_paths)} completed successfully")
            except Exception as e:
                logger.error(f"‚ö†Ô∏è Problem with PDF {i}/{len(pdf_paths)} [{os.path.basename(pdf_path)}]: {e}")
                results[pdf_path] = {
                    'success': False,
                    'error': str(e)
                }
                # Continuer avec le PDF suivant (comme demand√©)
                continue
        
        return results
    
    def _process_pdfs_parallel(self, pdf_paths: List[str], max_workers: int = 3) -> Dict[str, Dict]:
        """Traitement parall√®le des PDF"""
        results = {}
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Soumettre les t√¢ches
            future_to_path = {
                executor.submit(self.extract_text_from_pdf, pdf_path): pdf_path
                for pdf_path in pdf_paths
            }
            
            # Collecter les r√©sultats
            for future in as_completed(future_to_path):
                pdf_path = future_to_path[future]
                try:
                    text, metadata, doc_stats = future.result()
                    results[pdf_path] = {
                        'success': True,
                        'data': (text, metadata, doc_stats)
                    }
                except Exception as e:
                    results[pdf_path] = {
                        'success': False,
                        'error': str(e)
                    }
        
        return results
    
    def get_processing_summary(self) -> str:
        """
        Retourne un r√©sum√© du traitement
        
        Returns:
            R√©sum√© format√© du traitement
        """
        stats = self.processing_stats
        
        summary = f"""
üìä R√©sum√© du traitement PDF:
‚Ä¢ Fichiers trait√©s: {stats['successful_files']}/{stats['total_files']}
‚Ä¢ Pages trait√©es: {stats['total_pages']}
‚Ä¢ Caract√®res extraits: {stats['total_characters']:,}
‚Ä¢ Fichiers en erreur: {stats['failed_files']}
"""
        
        if stats['total_characters'] > 0:
            avg_chars_per_page = stats['total_characters'] / max(stats['total_pages'], 1)
            summary += f"‚Ä¢ Moyenne par page: {avg_chars_per_page:.0f} caract√®res\n"
        
        return summary.strip()

class PDFTextChunker:
    """
    Gestionnaire de chunking pour le texte extrait des PDF
    """
    
    def __init__(self, chunk_size: int = None, chunk_overlap: int = None):
        """
        Initialise le chunker
        
        Args:
            chunk_size: Taille des chunks (utilise settings par d√©faut)
            chunk_overlap: Chevauchement entre chunks (utilise settings par d√©faut)
        """
        self.chunk_size = chunk_size or settings.CHUNK_SIZE
        self.chunk_overlap = chunk_overlap or settings.CHUNK_OVERLAP
    
    def create_chunks_with_metadata(self, text: str, metadata: Dict) -> Tuple[List[str], Dict]:
        """
        Cr√©e des chunks de texte en pr√©servant les m√©tadonn√©es
        
        Args:
            text: Texte √† chunker
            metadata: M√©tadonn√©es associ√©es
            
        Returns:
            Tuple (liste_des_chunks, m√©tadonn√©es_des_chunks)
        """
        if not text.strip():
            logger.warning("Empty text provided for chunking")
            return [], {}
        
        # Import des outils de chunking
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        from utils.constants import TEXT_SEPARATORS
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=TEXT_SEPARATORS
        )
        
        chunks = text_splitter.split_text(text)
        chunk_metadata = {}
        
        logger.info(f"Text chunking: {len(text)} chars ‚Üí {len(chunks)} chunks")
        
        for i, chunk in enumerate(chunks):
            # Trouver les m√©tadonn√©es associ√©es √† ce chunk
            best_match = self._find_chunk_metadata(chunk, metadata)
            
            chunk_metadata[i] = best_match or {
                'document': 'Document inconnu',
                'page': 'Page inconnue',
                'text_length': len(chunk),
                'chunk_index': i,
                'chunk_size': self.chunk_size
            }
        
        logger.info(f"Chunking completed: {len(chunks)} chunks created")
        return chunks, chunk_metadata
    
    def _find_chunk_metadata(self, chunk: str, metadata: Dict) -> Optional[Dict]:
        """
        Trouve les m√©tadonn√©es correspondant √† un chunk
        
        Args:
            chunk: Chunk de texte
            metadata: M√©tadonn√©es disponibles
            
        Returns:
            M√©tadonn√©es correspondantes ou None
        """
        import re
        
        # Chercher les marqueurs de m√©tadonn√©es dans le chunk
        pattern = r'\[DOC_META_(\d+)\]'
        matches = re.findall(pattern, chunk)
        
        if matches:
            # Utiliser le premier marqueur trouv√©
            meta_id = int(matches[0])
            return metadata.get(meta_id)
        
        return None

# Fonctions utilitaires pour compatibilit√©
def get_pdf_text_with_metadata_robust(pdf_paths: List[str]) -> Tuple[str, Dict]:
    """
    Fonction de compatibilit√© avec l'ancienne interface
    
    Args:
        pdf_paths: Liste des chemins PDF
        
    Returns:
        Tuple (texte, m√©tadonn√©es)
    """
    processor = PDFProcessor(validate_files=True, parallel_processing=False)
    text, metadata, _ = processor.extract_text_from_multiple_pdfs(pdf_paths)
    return text, metadata

def get_text_chunks_with_metadata(text: str, metadata: Dict) -> Tuple[List[str], Dict]:
    """
    Fonction de compatibilit√© pour le chunking
    
    Args:
        text: Texte √† chunker
        metadata: M√©tadonn√©es
        
    Returns:
        Tuple (chunks, m√©tadonn√©es_chunks)
    """
    chunker = PDFTextChunker()
    return chunker.create_chunks_with_metadata(text, metadata)
